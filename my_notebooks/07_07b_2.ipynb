{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7252adbf-4476-49ed-b1bb-c51604a2e729",
   "metadata": {},
   "source": [
    "## Asynchronous scraping"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb191576-cfe0-4823-aeb8-2ce0ed7a1cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:55:41.182821Z",
     "start_time": "2024-09-18T13:55:40.998428Z"
    }
   },
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:55:41.188124Z",
     "start_time": "2024-09-18T13:55:41.184330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "id": "ded747995b980fbe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:55:41.194183Z",
     "start_time": "2024-09-18T13:55:41.189581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrap_and_save_links(text):\n",
    "    souap = BeautifulSoup(text, 'html.parser')\n",
    "    file = open('csv.file', 'a', newline='')\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    for link in souap.findAll(arrts={'href': re.compile('^http')}):\n",
    "        link = link.get('href')\n",
    "        writer.writerow([link])\n",
    "        \n",
    "    file.close()"
   ],
   "id": "927537289bcde3fb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:55:55.568593Z",
     "start_time": "2024-09-18T13:55:55.561367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch(session, url):\n",
    "    try:\n",
    "         with session.get(url) as response:\n",
    "            text = response.text()\n",
    "            scrap_and_save_links(text)\n",
    "            #await task\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ],
   "id": "1bf49403529be539",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:56:35.850141Z",
     "start_time": "2024-09-18T13:56:35.843139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrap(urls):\n",
    "    tasks = [] \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in urls:\n",
    "            fetch(session, url)"
   ],
   "id": "8be57c9a55ba8b28",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'async with' outside async function (3163105830.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[8], line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    async with aiohttp.ClientSession() as session:\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m 'async with' outside async function\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T13:56:01.431209Z",
     "start_time": "2024-09-18T13:56:01.189779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "urls = ['https://www.google.com', 'https://www.yahoo.com', 'https://www.analytics.com', 'https://www.python.org', 'https://www.linkedin.com']\n",
    "\n",
    "scrap(urls)"
   ],
   "id": "3834b71fe502c9f2",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Use async with instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m urls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.google.com\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.yahoo.com\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.analytics.com\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.python.org\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.linkedin.com\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 3\u001B[0m \u001B[43mscrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43murls\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m, in \u001B[0;36mscrap\u001B[0;34m(urls)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscrap\u001B[39m(urls):\n\u001B[1;32m      2\u001B[0m     tasks \u001B[38;5;241m=\u001B[39m [] \n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m aiohttp\u001B[38;5;241m.\u001B[39mClientSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m urls:\n\u001B[1;32m      5\u001B[0m             fetch(session, url)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/aiohttp/client.py:1306\u001B[0m, in \u001B[0;36mClientSession.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1306\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse async with instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Use async with instead"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T14:07:00.714392Z",
     "start_time": "2024-09-18T14:07:00.708054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrap_and_save_links2(text):\n",
    "    print('a')\n",
    "    souap = BeautifulSoup(text, 'html.parser')\n",
    "    file = open('csv2.file', 'a', newline='')\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    for link in souap.find_all(attrs={'href': re.compile('^http')}):\n",
    "        link = link.get('href')\n",
    "        print(link)\n",
    "        writer.writerow([link])\n",
    "        \n",
    "    file.close()\n",
    "    \n",
    "scrap_and_save_links2('<a href=\"https://www.google.com\">Google</a> <a href=\"https://www.google2.com\">Google</a>')"
   ],
   "id": "6d9b7a2bae05b788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "https://www.google.com\n",
      "https://www.google2.com\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
